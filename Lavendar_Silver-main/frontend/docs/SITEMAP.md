# Sitemap & SEO

## Sitemap location and URL

- **File in project:** `frontend/public/sitemap.xml`
- **Public URL:** `https://pvjewellers.in/sitemap.xml` (or your domain + `/sitemap.xml`)

The file is generated by script and copied as-is into the build output, so it is served at the root: `/sitemap.xml`.

---

## What is included / excluded

**Included (indexable):**

- Static pages: home, shop, about, contact, blogs, FAQ, policies, video-cart, old-gold, goldmine, digital-gold, custom-jewelry, wishlist
- All product pages: `/product/{slug}` (from API)
- All published blog posts: `/blog/{slug}` (from API)

**Excluded (not in sitemap):**

- Admin: `/admin/*`
- Auth: `/login`, `/signup`
- Cart & checkout: `/carts`, `/checkout`, `/thankyou`
- Private: `/myaccount`, `/video-cart/booking`, `/video-cart/thankyou`
- Compare: `/compare/*`

These excluded paths are also listed in `robots.txt` as `Disallow` so crawlers do not waste crawl budget on them.

---

## How the sitemap is generated (programmatic)

The sitemap is **generated at build time** (and can be run manually), so it updates when:

- You add/remove static routes in `src/config/sitemapRoutes.js`
- The API returns new/updated products or blogs (script fetches slugs from the API)

**Where things live:**

| Item | Location |
|------|----------|
| Static route list (indexable only) | `frontend/src/config/sitemapRoutes.js` |
| Generator script | `frontend/scripts/generate-sitemap.js` |
| Output file | `frontend/public/sitemap.xml` |
| robots.txt | `frontend/public/robots.txt` |

**How it’s triggered:**

1. **Automatically before each build**  
   `npm run build` in the frontend runs the `prebuild` script, which runs `npm run generate-sitemap`. So every production build gets a fresh sitemap (static + current products/blogs from API).

2. **Manually**  
   From the frontend directory:
   ```bash
   npm run generate-sitemap
   ```
   This fetches product and blog slugs from the API (if `VITE_API_URL` is set), merges them with static routes, and overwrites `public/sitemap.xml`.

**Environment:**

- `SITE_URL` – base URL (e.g. `https://pvjewellers.in`). Default: `https://pvjewellers.in`.
- `VITE_API_URL` – API base for fetching products/blogs (e.g. `https://backend.pvjewellers.in`). If missing, only static routes are included.

---

## Google sitemap standards (what we follow)

- **`<loc>`** – Absolute, canonical URL (no trailing slash except for home).
- **`<lastmod>`** – Date in `YYYY-MM-DD` (ISO 8601).
- **`<changefreq>`** – One of: `always`, `hourly`, `daily`, `weekly`, `monthly`, `yearly`.
- **`<priority>`** – Number between 0.0 and 1.0 (relative hint for crawlers).
- XML encoding UTF-8; schema: `http://www.sitemaps.org/schemas/sitemap/0.9`.

---

## Submitting the sitemap to Google Search Console

1. **Open Google Search Console**  
   Go to [Google Search Console](https://search.google.com/search-console) and select your property (e.g. `https://pvjewellers.in`).

2. **Go to Sitemaps**  
   In the left sidebar: **Indexing** → **Sitemaps**.

3. **Add the sitemap URL**  
   In “Add a new sitemap” enter:
   ```text
   sitemap.xml
   ```
   (Only the path; Google will use your property URL as base.)

4. **Submit**  
   Click **Submit**. Status will show as “Success” or “Couldn’t fetch” (fix errors if any).

5. **Optional – request indexing**  
   After submitting, you can use **URL Inspection** for important URLs and request indexing.

**Notes:**

- Ensure `https://pvjewellers.in/sitemap.xml` is reachable (no auth, returns 200 and valid XML).
- Ensure `robots.txt` is live at `https://pvjewellers.in/robots.txt` and contains:
  ```text
  Sitemap: https://pvjewellers.in/sitemap.xml
  ```
- After adding new products or blogs, run a new build (or `npm run generate-sitemap`) and redeploy so the sitemap updates; Google will pick it up on the next crawl.

---

## Updating the sitemap when the site changes

- **New static page:** Add it to `SITEMAP_STATIC_ROUTES` in `src/config/sitemapRoutes.js` (path, `changefreq`, `priority`). Do **not** add admin, login, cart, checkout, or private pages.
- **New product or blog:** No code change; they are pulled from the API when you run `generate-sitemap` (or on each build).
- **New domain:** Set `SITE_URL` in env (or in `.env`) and ensure `robots.txt` uses the same domain in the `Sitemap:` line (you can make `robots.txt` dynamic or update it once per domain).
